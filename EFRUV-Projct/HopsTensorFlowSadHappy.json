{"paragraphs":[{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482856894748_-907959262","id":"20161227-174134_1914400613","dateCreated":"2016-12-27T05:41:34+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:98","user":"demo_paulvel0__paulvelt","dateUpdated":"2016-12-27T17:43:54+0100","dateFinished":"2016-12-27T17:43:56+0100","dateStarted":"2016-12-27T17:43:54+0100","result":{"code":"SUCCESS","type":"TEXT","msg":""},"text":"#! /usr/bin/env python","focus":true},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482856913992_1644495589","id":"20161227-174153_597946689","dateCreated":"2016-12-27T17:41:53+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:258","text":"import tensorflow as tf\nimport numpy as np\nimport os\nfrom PIL import Image\nimport random\nimport matplotlib\nmatplotlib.use('Agg') #To use matplotlib in headless mode\nimport matplotlib.pyplot as plt","user":"demo_paulvel0__paulvelt","dateUpdated":"2016-12-27T17:58:34+0100","dateFinished":"2016-12-27T17:58:35+0100","dateStarted":"2016-12-27T17:58:35+0100","result":{"code":"SUCCESS","type":"TEXT","msg":""}},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482857109528_404558798","id":"20161227-174509_1823751627","dateCreated":"2016-12-27T17:45:09+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1075","user":"demo_paulvel0__paulvelt","dateUpdated":"2016-12-27T17:58:41+0100","dateFinished":"2016-12-27T17:58:42+0100","dateStarted":"2016-12-27T17:58:42+0100","result":{"code":"SUCCESS","type":"TEXT","msg":""},"text":"def transformImage(file):\n    \"\"\"Transforming a image into a numpy array\n    params: The image file\n    Returns the transformed image as numpy array in the dimension (widht, length, color)\"\"\"\n    im = Image.open(file)\n    pix = im.load()\n    width = im.size[0]\n    height = im.size[1]\n    #store the rgb information in a numpy array (width, height, color).\n    picture = np.array([[pix[x,y] for y in range(height)] for x in range(width)], np.int32)\n    return picture\n\n\ndef createDataSets(smilePath, nonSmilePath, dataSetSize, testingSplit):\n    \"\"\"Createts the training and test datasets from the images in smilePath and nonSmilePath.\n    Params: The path the the smiling images, the path to the non smiling images, the size of the dataSet we want to use\n    The split value for the testing and training set. The split is set based on the testing split size in percent.\n    If a testing split of 20 is chosen 20% are going to be test data and 80% training data.\n    Returns: the testing and training labels, the trainingSet and TestingSet\n    \"\"\"\n    trainingLabels = []\n    trainingSetFiles = []\n    testingLabels = []\n    testingSet = []\n    # transform all smiling pictures\n    for root, dirs, files in os.walk(smilePath, True):\n        i=0\n        #static for loop\n        for name in files:\n        #all images\n        #for name in files:\n            if name.endswith(\".jpg\") and (i<(dataSetSize/2) or dataSetSize == -1):\n                if random.randint(1, 100) > testingSplit:\n                    trainingSetFiles.append(os.path.join(root, name))\n                    trainingLabels.append(np.array([1,0], np.int32))\n                else:\n                    testingSet.append(transformImage(os.path.join(root, name)))\n                    testingLabels.append(np.array([1,0], np.int32))\n                i=i+1\n    # transform all non-smiling pictures\n    #the non smiling pictures are added to a random position in the trainingSet and labels and the testingSet and labels\n    #the sets and labelled where already created in the above for loop. \n    for root, dirs, files in os.walk(nonSmilePath, True):\n        k=0\n        #all images\n        #for name in files:\n        #static for loop\n        for name in files:\n            if name.endswith(\".jpg\") and (k<(dataSetSize/2) or dataSetSize == -1):\n                if random.randint(1, 100) > testingSplit:\n                    # insert to a random position to avoid overfitting\n                    insertPosition = random.randint(0, len(trainingLabels))\n                    trainingSetFiles.insert(insertPosition, os.path.join(root, name))\n                    trainingLabels.insert(insertPosition, np.array([0, 1], np.int32))\n                else:\n                    # insert to a random position to avoid overfitting\n                    insertPosition = random.randint(0, len(trainingLabels))\n                    testingSet.insert(insertPosition, transformImage(os.path.join(root, name)))\n                    testingLabels.insert(insertPosition, np.array([0, 1], np.int32))\n                k=k+1\n    return trainingSetFiles,trainingLabels,testingSet,testingLabels\n    #TODO: Needs to be explained better Side note: Only the file names of the training images are provided to reduce memory consumption.\n\ndef tensorFlowModel(trainingSet,trainingLabels,testingSet,testingLabels,batchSize):\n    \"\"\"The actual tensorflow Model\n    Here we initialise first all the variables, such as the numpy images, the labels, the learning rate, the weights, the bayes. \n    We then initialize a convolutional layer and initialize ReLu and max pooling\n    Then we reshape it for the processing\n    We perform a dropout to make the analysis perform faster and prevent overfitting\n    The model will then be trained in batches.\n    Each batch has a TrainingSet and a testingSet, For each training Image an iteration will be performed\n    The training session uses the dropout and will be trained, the testing session not. \n    After all the batches are finished there will be a train result for the accuracy and the loss. \n    There is also a testing result for the accuracy and the loss\n    Params: The trainingSet, trainingLabels, the TestingLabels and the TestingSet, The batch size the model will be trained in\n    Returns: the training accuracy, the training loss, the testing accuracy, the testing loss\n    \"\"\"\n    #placeholder for the numpy images\n    X = tf.placeholder(tf.float32, [None, 320, 240, 3])\n    #placeholder for the labels\n    Y_ = tf.placeholder(tf.float32, [None, 2])\n    #placeholder for the learning rate. \n    learning_rate = tf.placeholder(tf.float32)\n    keep_prob = tf.placeholder(tf.float32)  # dropout (keep probability)\n    # weight 1\n    W1 = tf.Variable(tf.truncated_normal([5, 5, 3, 12], stddev=0.1))\n    # bias\n    B1 = tf.Variable(tf.ones([12]) / 3)\n    # convolutional layer 1\n    Y1 = tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding='SAME')\n    #Initialize the Relu layer\n    Y1Relu = tf.nn.relu(Y1 + B1)\n    # max layer pooling\n    pool1 = tf.nn.max_pool(Y1Relu, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')  # halve structure to 160x120\n    # reshape\n    YY = tf.reshape(pool1, shape=[-1, 120 * 160 * 12])  # 160*120 structure\n    # Apply Dropout\n    YY = tf.nn.dropout(YY, keep_prob)\n    # weight 2\n    W2 = tf.Variable(tf.truncated_normal([160 * 120 * 12, 2], stddev=0.1))\n    # bias\n    B2 = tf.Variable(tf.ones([2]) / 3)\n    # softmax\n    Ylogits = tf.matmul(YY, W2) + B2\n    Y = tf.nn.softmax(Ylogits)\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(Ylogits, Y_)\n    cross_entropy = tf.reduce_mean(cross_entropy)\n    # accuracy of the trained model, between 0 (worst) and 1 (best)\n    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n    init = tf.global_variables_initializer()\n    sess = tf.Session()\n    sess.run(init)\n    # set learning rate and learning rate decay\n    initial_learning_rate = 0.0005\n    decay_rate = 0.95\n    decay_steps = 500\n    stepCounter = 0\n    # set dropout keep probability\n    dropout = 0.80\n    # sets to plot\n    train_a = []\n    train_c = []\n    test_a = []\n    test_c = []\n    # train the model in batches\n    for step in range(0,len(trainingSet),batchSize):\n        #reduce the learning rate\n        if stepCounter > decay_steps:\n            lr = initial_learning_rate * decay_rate ** (int(stepCounter / decay_steps))\n        else:\n            lr = initial_learning_rate\n        # use the next batch\n        batchBegin = step\n        batchEnd = step+batchSize\n        if batchEnd > len(trainingSet):\n            batchEnd = len(trainingSet)\n        # generate the next batch\n        nextBatch = []\n        for image in trainingSet[batchBegin:batchEnd]:\n            nextBatch.append(transformImage(image))\n        batch_X = np.asarray(nextBatch)\n        del nextBatch[:]\n        batch_Y = np.asarray(trainingLabels[batchBegin:batchEnd])\n        # train\n        sess.run(train_step, feed_dict={X: batch_X, Y_: batch_Y, learning_rate:lr, keep_prob:dropout})\n        a, c = sess.run([accuracy, cross_entropy], feed_dict={X: batch_X, Y_: batch_Y, keep_prob:1.0})\n        train_a.append(a)\n        train_c.append(c)\n        a, c = sess.run([accuracy, cross_entropy], feed_dict={X: testingSet, Y_: testingLabels, keep_prob:1.0})\n        test_a.append(a)\n        test_c.append(c)\n        stepCounter += 1\n    return train_a, train_c, test_a, test_c\n\ndef plotResults(train_a, test_a, train_c, test_c):\n    \"\"\"\n    Plot and visualise the accuracy and loss\n    accuracy training vs testing dataset\n    Params: the training accuracy, testing accuracy, training loss, testing loss\n    Returns the plot for the accuracy and the loss as an image\n    \"\"\"\n    plt.plot(train_a, label='training dataset')\n    plt.plot(test_a, label='test dataset')\n    plt.legend(bbox_to_anchor=(0, 0.95), loc='lower left', ncol=1)\n    plt.xlabel('# batch')\n    plt.ylabel('accuracy')\n    plt.grid(True)\n    #plt.show()\n    plt.savefig('accuracy.png')\n    plt.clf()\n    # loss training vs testing dataset\n    plt.plot(train_c, label='training dataset')\n    plt.plot(test_c, label='test dataset')\n    plt.legend(bbox_to_anchor=(0, 0.95), loc='lower left', ncol=1)\n    plt.xlabel('# batch')\n    plt.ylabel('loss')\n    plt.grid(True)\n    #plt.show()\n    plt.savefig('loss.png')\n"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482857534015_-749109396","id":"20161227-175214_1225264544","dateCreated":"2016-12-27T17:52:14+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3323","user":"demo_paulvel0__paulvelt","dateUpdated":"2016-12-27T17:58:49+0100","dateFinished":"2016-12-27T17:58:51+0100","dateStarted":"2016-12-27T17:58:50+0100","result":{"code":"SUCCESS","type":"TEXT","msg":"                        size of training set: 0 0\nsize of testing set: 0 0\nTraceback (most recent call last):\n  File \"<stdin>\", line 2, in <module>\n  File \"<stdin>\", line 18, in main\n  File \"<stdin>\", line 49, in tensorFlowModel\nAttributeError: 'module' object has no attribute 'global_variables_initializer'\n"},"text":"def main(argv=None):\n    \"\"\"\n    In the main function we initialize the datasetSize, the testing split and the batch size\n    The DataSetSize says how much images we of each set want to use, for example of happy pictues and non happy pictures\n    The batchsize defines the size used for training\n    When this is initialized we create the Datasets, here we obtain the training set and the testing set\n    We run our tensorflow model\n    We then visualize the outcome of our tensorflow model, by plotting the result\n    Returns: The plots created in the plotResults function. \n    \"\"\"\n    dataSetSize = 750 # use -1 for all images\n    testingSplit = 20 # in % of total data-set size\n    batchSize = 25\n    trainingSetFiles, trainingLabels, testingSet, testingLabels = createDataSets(\"AMFED/AMFED/happiness/\",\"AMFED/AMFED/nonHappiness/\",dataSetSize,testingSplit)\n    #batchSize = len(testingSet) #to train in batches of the testing set size\n    print \"size of training set:\", len(trainingSetFiles), len(trainingLabels)\n    print \"size of testing set:\", len(testingSet), len(testingLabels)\n    train_a, train_c, test_a, test_c = tensorFlowModel(trainingSetFiles,trainingLabels,testingSet,testingLabels,batchSize)\n    #print \"Training and Testing - Accurracy, Cross Entropy:\"\n    #print train_a, train_c\n    #print test_a, test_c\n    plotResults(train_a, test_a, train_c, test_c)\n\nif __name__ == '__main__':\n    main()"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1482857865967_501230940","id":"20161227-175745_1922055860","dateCreated":"2016-12-27T17:57:45+0100","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6301","dateUpdated":"2016-12-27T17:57:56+0100","text":""}],"name":"TensorFlowSadHappy","id":"2C74SE1W1","angularObjects":{"2BSMV3RNQ:shared_process":[],"2BRKFTME2:shared_process":[],"2BRWND7YH:shared_process":[],"2BRBC89PN:shared_process":[],"2BR8UJC54:shared_process":[],"2BTHHFNS8:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}